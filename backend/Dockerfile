# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DOCKERFILE PARA RENDER - SISTEMA HÃBRIDO OLLAMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# Este Dockerfile despliega la aplicaciÃ³n en Render con soporte para:
# - Node.js 18
# - Ollama 3B (modelo pequeÃ±o para 2 GB RAM)
# - PostgreSQL (conexiÃ³n externa)
# - Sistema hÃ­brido con fallback a OpenAI
#
# IMPORTANTE: Ollama 3B funciona en Render Starter (2 GB RAM)
# Para mejor precisiÃ³n, considera usar servidor externo de Ollama o OpenAI
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FROM node:18-slim

# Metadatos
LABEL maintainer="Sistema BiomÃ©trico"
LABEL description="Backend con Ollama 3B para diagnÃ³stico IA"
LABEL version="2.0.0"

# Variables de entorno por defecto
ENV NODE_ENV=production
ENV PORT=10000
ENV OLLAMA_BASE_URL=http://localhost:11434
ENV OLLAMA_MODEL=llama3.1:3b
ENV OLLAMA_TIMEOUT=30000

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Instalar Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Crear directorio de la aplicaciÃ³n
WORKDIR /app

# Copiar package files
COPY package*.json ./

# Instalar dependencias de Node
RUN npm ci --only=production

# Copiar cÃ³digo fuente
COPY . .

# Exponer puerto
EXPOSE 10000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:${PORT}/api/v1/health || exit 1

# Script de inicio
RUN echo '#!/bin/bash\n\
set -e\n\
\n\
echo "ðŸš€ Iniciando aplicaciÃ³n en Render..."\n\
\n\
# Iniciar Ollama en background\n\
echo "ðŸ¤– Iniciando Ollama service..."\n\
ollama serve &\n\
OLLAMA_PID=$!\n\
\n\
# Esperar a que Ollama estÃ© listo\n\
echo "â³ Esperando a que Ollama estÃ© listo..."\n\
sleep 10\n\
\n\
# Descargar modelo 3B (pequeÃ±o para 2 GB RAM)\n\
echo "ðŸ“¥ Descargando modelo llama3.1:3b..."\n\
if ! ollama list | grep -q "llama3.1:3b"; then\n\
  ollama pull llama3.1:3b || echo "âš ï¸  No se pudo descargar modelo, continuando..."\n\
fi\n\
\n\
# Verificar Ollama\n\
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then\n\
  echo "âœ… Ollama estÃ¡ corriendo"\n\
else\n\
  echo "âš ï¸  Ollama no disponible, se usarÃ¡ fallback (OpenAI o Patterns)"\n\
fi\n\
\n\
# Iniciar Node.js\n\
echo "ðŸš€ Iniciando Node.js en puerto $PORT..."\n\
exec node server.js\n\
' > /app/start.sh && chmod +x /app/start.sh

# Comando de inicio
CMD ["/app/start.sh"]
