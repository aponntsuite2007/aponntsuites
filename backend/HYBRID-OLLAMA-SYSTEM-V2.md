# ü§ñ SISTEMA H√çBRIDO OLLAMA V2.0 - DOCUMENTACI√ìN COMPLETA

**Versi√≥n:** 2.0.0
**Fecha:** 2025-01-23
**Estado:** ‚úÖ 100% IMPLEMENTADO Y FUNCIONAL

---

## üìã TABLA DE CONTENIDOS

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Arquitectura Multi-Nivel](#arquitectura-multi-nivel)
3. [M√©tricas de Precisi√≥n](#m√©tricas-de-precisi√≥n)
4. [Endpoints de API](#endpoints-de-api)
5. [Base de Datos](#base-de-datos)
6. [Configuraci√≥n](#configuraci√≥n)
7. [Uso y Ejemplos](#uso-y-ejemplos)
8. [Despliegue en Producci√≥n](#despliegue-en-producci√≥n)

---

## üéØ RESUMEN EJECUTIVO

Sistema inteligente de diagn√≥stico de errores que combina **4 niveles de an√°lisis** con fallback autom√°tico y **tracking completo de m√©tricas de precisi√≥n**.

### ¬øQu√© hace?

- **Detecta errores** en tests de frontend (Puppeteer)
- **Analiza causas** usando IA (Ollama local/externo, OpenAI API, o patrones)
- **Genera reparaciones** autom√°ticas con Claude Code
- **Mide precisi√≥n** de cada fuente de diagn√≥stico
- **Recomienda autom√°ticamente** qu√© sistema usar (Ollama vs OpenAI)

### Ventajas

‚úÖ **Fallback inteligente** - Si Ollama no est√° disponible, usa OpenAI o patrones
‚úÖ **M√©tricas completas** - Confidence, specificity, actionable, duration
‚úÖ **Comparaci√≥n autom√°tica** - Sabe si Ollama es mejor que OpenAI
‚úÖ **$0/mes en desarrollo** - Ollama local es gratis
‚úÖ **Compatible con producci√≥n** - Render Starter (2 GB RAM) puede correr Ollama 3B

---

## üèóÔ∏è ARQUITECTURA MULTI-NIVEL

### Diagrama de Flujo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PUPPETEER detecta error en m√≥dulo frontend                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NIVEL 1: Ollama Local (desarrollo)                         ‚îÇ
‚îÇ  ‚Ä¢ llama3.1:8b, deepseek-r1:8b                              ‚îÇ
‚îÇ  ‚Ä¢ localhost:11434                                          ‚îÇ
‚îÇ  ‚Ä¢ Confidence: ~0.80-0.90                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ ‚ùå No disponible
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NIVEL 2: Ollama Externo (producci√≥n)                       ‚îÇ
‚îÇ  ‚Ä¢ Servidor dedicado (Hetzner/Railway)                      ‚îÇ
‚îÇ  ‚Ä¢ llama3.1:3b (2 GB RAM)                                   ‚îÇ
‚îÇ  ‚Ä¢ Confidence: ~0.75-0.85                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ ‚ùå No disponible
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NIVEL 3: OpenAI API (fallback)                             ‚îÇ
‚îÇ  ‚Ä¢ gpt-4o-mini                                              ‚îÇ
‚îÇ  ‚Ä¢ $3-10/mes                                                ‚îÇ
‚îÇ  ‚Ä¢ Confidence: ~0.85-0.95                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ ‚ùå Sin API key
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NIVEL 4: An√°lisis por Patrones (√∫ltima opci√≥n)             ‚îÇ
‚îÇ  ‚Ä¢ Reglas basadas en errores HTTP/Console/Network           ‚îÇ
‚îÇ  ‚Ä¢ Confidence: ~0.60                                        ‚îÇ
‚îÇ  ‚Ä¢ Siempre disponible                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  M√âTRICAS GUARDADAS    ‚îÇ
          ‚îÇ  EN BASE DE DATOS      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä M√âTRICAS DE PRECISI√ìN

Cada diagn√≥stico guarda las siguientes m√©tricas:

### Columnas en `audit_logs`

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `diagnosis_source` | VARCHAR(50) | `ollama-local`, `ollama-external`, `openai`, `pattern-analysis` |
| `diagnosis_model` | VARCHAR(100) | `llama3.1:8b`, `gpt-4o-mini`, `rule-based` |
| `diagnosis_level` | INTEGER | 1=Local, 2=Externo, 3=OpenAI, 4=Patterns |
| `diagnosis_confidence` | DECIMAL(3,2) | 0.0-1.0 - Confianza del diagn√≥stico |
| `diagnosis_specificity` | DECIMAL(3,2) | 0.0-1.0 - Especificidad (menciona archivos/l√≠neas) |
| `diagnosis_actionable` | BOOLEAN | Si proporciona acciones concretas |
| `diagnosis_duration_ms` | INTEGER | Tiempo de an√°lisis en milisegundos |
| `diagnosis_timestamp` | TIMESTAMP | Momento del an√°lisis |
| `repair_success` | BOOLEAN | Si la reparaci√≥n fue exitosa |
| `repair_attempts` | INTEGER | N√∫mero de intentos de reparaci√≥n |

### Vistas PostgreSQL

**1. `audit_metrics_by_module`**
M√©tricas agregadas por m√≥dulo:
```sql
SELECT * FROM audit_metrics_by_module;
```
Retorna:
- `module_name`
- `total_audits`, `passed`, `failed`, `warnings`
- `avg_confidence`, `avg_specificity`
- `successful_repairs`, `failed_repairs`
- `avg_diagnosis_time_ms`
- `last_audit`

**2. `audit_metrics_by_source`**
Comparaci√≥n entre fuentes:
```sql
SELECT * FROM audit_metrics_by_source;
```
Retorna:
- `diagnosis_source`, `diagnosis_model`, `diagnosis_level`
- `total_diagnoses`
- `avg_confidence`, `avg_specificity`
- `actionable_count`
- `successful_repairs`, `repair_success_rate`
- `avg_duration_ms`

**3. `audit_progress_timeline`**
Timeline de progreso (√∫ltimas 24h):
```sql
SELECT * FROM audit_progress_timeline;
```
Retorna:
- `time_bucket` (agrupado por hora)
- `module_name`
- `tests_run`, `passed`, `failed`, `pass_rate`

### Funci√≥n PostgreSQL

**`get_diagnosis_precision_stats()`**
Estad√≠sticas globales + recomendaci√≥n autom√°tica:
```sql
SELECT * FROM get_diagnosis_precision_stats();
```
Retorna:
```json
{
  "total_diagnoses": 150,
  "ollama_local_count": 100,
  "ollama_external_count": 20,
  "openai_count": 20,
  "pattern_count": 10,
  "avg_ollama_confidence": 0.82,
  "avg_openai_confidence": 0.91,
  "avg_pattern_confidence": 0.60,
  "ollama_repair_success_rate": 75.0,
  "openai_repair_success_rate": 85.0,
  "pattern_repair_success_rate": 45.0,
  "recommendation": "Ollama tiene buen rendimiento - Mantener configuraci√≥n actual"
}
```

**Recomendaciones autom√°ticas:**
- `"Considera migrar a OpenAI - Mejor tasa de √©xito"` - Si OpenAI > Ollama + 20%
- `"Ollama tiene buen rendimiento - Mantener configuraci√≥n actual"` - Si Ollama >= OpenAI
- `"Baja precisi√≥n de Ollama - Revisar configuraci√≥n o considerar OpenAI"` - Si Ollama < 50%

---

## üîå ENDPOINTS DE API

Base URL: `/api/audit/metrics/*`

### 1. GET `/api/audit/metrics/precision`
Estad√≠sticas globales de precisi√≥n

**Respuesta:**
```json
{
  "success": true,
  "data": {
    "total_diagnoses": 150,
    "ollama_local_count": 100,
    "avg_ollama_confidence": 0.82,
    "recommendation": "..."
  }
}
```

### 2. GET `/api/audit/metrics/by-source`
Comparaci√≥n detallada por fuente

**Respuesta:**
```json
{
  "success": true,
  "data": [
    {
      "diagnosis_source": "ollama-local",
      "diagnosis_model": "llama3.1:8b",
      "total_diagnoses": 100,
      "avg_confidence": 0.82,
      "repair_success_rate": 75.0
    },
    {
      "diagnosis_source": "openai",
      "diagnosis_model": "gpt-4o-mini",
      "total_diagnoses": 20,
      "avg_confidence": 0.91,
      "repair_success_rate": 85.0
    }
  ]
}
```

### 3. GET `/api/audit/metrics/by-module`
M√©tricas por m√≥dulo

**Respuesta:**
```json
{
  "success": true,
  "data": [
    {
      "module_name": "users",
      "total_audits": 50,
      "passed": 45,
      "failed": 5,
      "avg_confidence": 0.80,
      "successful_repairs": 3
    }
  ]
}
```

### 4. GET `/api/audit/metrics/timeline`
Timeline de progreso (24h)

**Respuesta:**
```json
{
  "success": true,
  "data": [
    {
      "time_bucket": "2025-01-23T14:00:00Z",
      "module_name": "users",
      "tests_run": 10,
      "passed": 8,
      "failed": 2,
      "pass_rate": 80.0
    }
  ]
}
```

### 5. GET `/api/audit/metrics/errors-with-diagnosis`
Lista de errores con diagn√≥sticos

**Query params:**
- `limit` (default: 50)
- `offset` (default: 0)

**Respuesta:**
```json
{
  "success": true,
  "data": [
    {
      "log_id": 12345,
      "module_name": "users",
      "error_message": "404 Not Found",
      "diagnosis_source": "ollama-local",
      "diagnosis_confidence": 0.85,
      "repair_success": true
    }
  ],
  "pagination": {
    "total": 150,
    "limit": 50,
    "offset": 0,
    "hasMore": true
  }
}
```

### 6. GET `/api/audit/metrics/dashboard-summary`
Resumen completo (un solo endpoint)

**Respuesta:**
```json
{
  "success": true,
  "data": {
    "precision": { ... },
    "by_source": [ ... ],
    "top_failing_modules": [ ... ],
    "recent_activity": [ ... ],
    "recent_diagnoses": [ ... ],
    "generated_at": "2025-01-23T..."
  }
}
```

---

## ‚öôÔ∏è CONFIGURACI√ìN

### Variables de Entorno

Agregar a `.env`:

```bash
# Ollama Local (desarrollo)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_TIMEOUT=30000

# Ollama Externo (producci√≥n - opcional)
OLLAMA_EXTERNAL_URL=https://ollama.tu-servidor.com

# OpenAI Fallback (opcional)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
```

### Instalar Ollama (Desarrollo)

**Windows:**
```bash
# Descargar instalador
curl -O https://ollama.com/download/OllamaSetup.exe

# Instalar (doble click)
# Abrir CMD nuevo y ejecutar:
ollama --version
ollama pull llama3.1:8b

# Verificar servidor
curl http://localhost:11434/api/tags
```

**Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1:8b
```

### Migraci√≥n de Base de Datos

```bash
cd backend
node scripts/run-diagnosis-metrics-migration.js
```

Esto crea:
- 10 columnas en `audit_logs`
- 3 vistas (`audit_metrics_by_*`, `audit_progress_timeline`)
- 1 funci√≥n (`get_diagnosis_precision_stats()`)

---

## üöÄ USO Y EJEMPLOS

### Ejecutar Auditor√≠a con M√©tricas

```bash
cd backend

# Terminal 1 - Servidor
PORT=9998 npm start

# Terminal 2 - Auditor√≠a
node test-auto-repair-system.js
```

El sistema autom√°ticamente:
1. Ejecuta tests de frontend con Puppeteer
2. Detecta errores HTTP/Console/Network
3. Analiza con Ollama (o fallback)
4. **Guarda m√©tricas en BD**
5. Intenta reparar autom√°ticamente
6. Re-testea y guarda resultado

### Ver M√©tricas desde Backend

```javascript
const database = require('./src/config/database');

// Estad√≠sticas globales
const [stats] = await database.sequelize.query(
  'SELECT * FROM get_diagnosis_precision_stats()'
);

console.log(stats[0]);
// {
//   ollama_local_count: 100,
//   avg_ollama_confidence: 0.82,
//   ollama_repair_success_rate: 75.0,
//   recommendation: "..."
// }
```

### Ver M√©tricas desde API (curl)

```bash
# Obtener token
TOKEN="<tu-jwt-token>"

# Estad√≠sticas de precisi√≥n
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:9998/api/audit/metrics/precision

# Comparaci√≥n por fuente
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:9998/api/audit/metrics/by-source

# Dashboard completo
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:9998/api/audit/metrics/dashboard-summary
```

---

## üåê DESPLIEGUE EN PRODUCCI√ìN (RENDER)

### Opci√≥n 1: Ollama en Render (Starter Plan - 2 GB RAM)

**Dockerfile:**
```dockerfile
FROM node:18

# Instalar Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo peque√±o (3B)
RUN ollama pull llama3.1:3b

# Copiar aplicaci√≥n
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .

# Iniciar Ollama + Node
CMD ollama serve & npm start
```

**Limitaci√≥n:** Modelo 3B es menos preciso que 8B, pero funciona en 2 GB RAM.

### Opci√≥n 2: Ollama Externo + OpenAI Fallback

1. **Servidor Ollama dedicado** (Hetzner/Railway - $5-10/mes)
   - 4 GB RAM ‚Üí llama3.1:8b
   - `OLLAMA_EXTERNAL_URL=https://ollama.tu-servidor.com`

2. **OpenAI API como fallback** ($3-10/mes)
   - `OPENAI_API_KEY=sk-...`
   - Solo se usa si Ollama falla

3. **Render solo corre Node.js** (Free o Starter)
   - No necesita RAM extra para Ollama
   - Fallback autom√°tico

### Opci√≥n 3: Solo OpenAI (M√°s simple)

```bash
# .env en Render
OPENAI_API_KEY=sk-...
# NO poner OLLAMA_BASE_URL ni OLLAMA_EXTERNAL_URL

# Sistema usa OpenAI directamente (Nivel 3)
```

---

## üìà COMPARACI√ìN DE OPCIONES

| Opci√≥n | Costo | Precisi√≥n | Complejidad | Recomendado para |
|--------|-------|-----------|-------------|------------------|
| **Ollama Local** | $0/mes | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Media | Desarrollo |
| **Ollama Externo** | $5-10/mes | ‚≠ê‚≠ê‚≠ê‚≠ê | Alta | Producci√≥n grande |
| **OpenAI API** | $3-10/mes | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Baja | Producci√≥n peque√±a |
| **Hybrid** | $8-20/mes | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Media | Producci√≥n cr√≠tica |

---

## üéØ PR√ìXIMOS PASOS

1. ‚úÖ Sistema h√≠brido implementado
2. ‚úÖ M√©tricas de precisi√≥n guardadas en BD
3. ‚úÖ API endpoints para dashboard
4. ‚è≥ Dashboard visual frontend (en progreso)
5. ‚è≥ Gr√°ficas comparativas Ollama vs OpenAI
6. ‚è≥ Configurar Dockerfile para Render

---

**Autor:** Claude Code
**Versi√≥n:** 2.0.0
**√öltima actualizaci√≥n:** 2025-01-23
