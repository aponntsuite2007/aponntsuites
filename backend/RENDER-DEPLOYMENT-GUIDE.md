# üöÄ GU√çA DE DESPLIEGUE EN RENDER - SISTEMA H√çBRIDO OLLAMA

**Versi√≥n:** 1.0.0
**Fecha:** 2025-01-23
**Para:** Render Starter Plan (2 GB RAM, $7/mes)

---

## üìã PREREQUISITOS

1. ‚úÖ Cuenta de Render (ya tienes)
2. ‚úÖ Repositorio Git del proyecto
3. ‚úÖ Base de datos PostgreSQL en Render
4. ‚ö†Ô∏è Ollama local instalado para desarrollo (opcional)

---

## üéØ OPCIONES DE DESPLIEGUE

### **OPCI√ìN 1: Ollama 3B en Render** (Recomendada si tienes Starter Plan)
- **Pros**: Todo en un solo servidor, $7/mes
- **Contras**: Modelo 3B es menos preciso que 8B
- **Precisi√≥n esperada**: ~75-85%

### **OPCI√ìN 2: OpenAI API** (M√°s simple)
- **Pros**: Mejor precisi√≥n (~90-95%), sin configuraci√≥n de Ollama
- **Contras**: $3-10/mes adicionales
- **Precisi√≥n esperada**: ~90-95%

### **OPCI√ìN 3: H√≠brido (Ollama Externo + OpenAI)**
- **Pros**: Mejor de ambos mundos
- **Contras**: $15-20/mes (Render + Hetzner + OpenAI)
- **Precisi√≥n esperada**: ~85-95%

---

## üöÄ PASO A PASO: OPCI√ìN 1 (Ollama 3B en Render)

### **1. Preparar Repositorio**

```bash
cd C:/Bio/sistema_asistencia_biometrico/backend

# Asegurarse que Dockerfile existe
ls -la Dockerfile

# Commit de archivos (si no lo hiciste)
git add Dockerfile .dockerignore
git commit -m "Add Dockerfile for Render deployment with Ollama 3B"
git push origin main
```

### **2. Configurar Servicio en Render**

1. **Ir a Render Dashboard**: https://dashboard.render.com
2. **Click "New +"** ‚Üí **"Web Service"**
3. **Conectar repositorio** (autorizar GitHub/GitLab)
4. **Seleccionar rama**: `main`

### **3. Configuraci√≥n del Servicio**

| Campo | Valor |
|-------|-------|
| **Name** | `aponntsuites-backend` |
| **Region** | Oregon (default) |
| **Branch** | `main` |
| **Root Directory** | `backend` |
| **Environment** | `Docker` |
| **Docker Command** | *(dejar vac√≠o, usa CMD del Dockerfile)* |
| **Plan** | `Starter` ($7/mes) |

### **4. Variables de Entorno**

Agregar estas variables en Render:

```bash
# Base de datos
DATABASE_URL=<tu-postgresql-url-de-render>

# Node
NODE_ENV=production
PORT=10000

# Ollama (se usa el local del contenedor)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:3b
OLLAMA_TIMEOUT=30000

# OpenAI Fallback (OPCIONAL pero recomendado)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# JWT
JWT_SECRET=<tu-secret-seguro>

# CORS
FRONTEND_URL=https://tu-frontend.com
```

### **5. Deploy**

1. Click **"Create Web Service"**
2. Render comenzar√° el build (15-20 minutos primera vez)
3. Ver√°s logs en tiempo real

**Logs esperados:**
```
üöÄ Iniciando aplicaci√≥n en Render...
ü§ñ Iniciando Ollama service...
‚è≥ Esperando a que Ollama est√© listo...
üì• Descargando modelo llama3.1:3b... (4-5 min)
‚úÖ Ollama est√° corriendo
üöÄ Iniciando Node.js en puerto 10000...
üöÄ Servidor corriendo en puerto 10000
```

### **6. Verificar Despliegue**

```bash
# Health check
curl https://aponntsuites.onrender.com/api/v1/health

# Verificar m√©tricas
curl -H "Authorization: Bearer <token>" \
  https://aponntsuites.onrender.com/api/audit/metrics/precision
```

---

## üöÄ PASO A PASO: OPCI√ìN 2 (Solo OpenAI API)

M√°s simple, mejor precisi√≥n, pero cuesta $3-10/mes adicionales.

### **1. Modificar Dockerfile**

Comentar la instalaci√≥n de Ollama:

```dockerfile
# Desactivar Ollama
# RUN curl -fsSL https://ollama.com/install.sh | sh

# Modificar start.sh para solo iniciar Node
CMD ["node", "server.js"]
```

### **2. Variables de Entorno en Render**

```bash
DATABASE_URL=<tu-postgresql-url>
NODE_ENV=production
PORT=10000
JWT_SECRET=<tu-secret>

# OpenAI como principal (no fallback)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# NO configurar OLLAMA_BASE_URL
```

El sistema autom√°ticamente:
- Detecta que Ollama no est√° disponible (Nivel 1 y 2 fallan)
- Va directo a OpenAI (Nivel 3)
- Si OpenAI falla, usa Patterns (Nivel 4)

---

## üöÄ PASO A PASO: OPCI√ìN 3 (H√≠brido Externo)

Mejor precisi√≥n, pero m√°s complejo.

### **1. Servidor Ollama Dedicado (Hetzner)**

**Crear VPS en Hetzner** ($5/mes - 4 GB RAM):

```bash
# SSH al servidor
ssh root@<ip-del-servidor>

# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo 8B
ollama pull llama3.1:8b

# Iniciar servicio (puerto 11434)
ollama serve

# Verificar
curl http://localhost:11434/api/tags
```

**Exponer p√∫blicamente** (con Nginx):

```nginx
server {
    listen 80;
    server_name ollama.tu-dominio.com;

    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### **2. Configurar en Render**

```bash
# Variables de entorno
DATABASE_URL=<tu-postgresql-url>
NODE_ENV=production
PORT=10000
JWT_SECRET=<tu-secret>

# Ollama externo
OLLAMA_EXTERNAL_URL=http://ollama.tu-dominio.com
OLLAMA_MODEL=llama3.1:8b

# OpenAI fallback
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
```

### **3. Deploy en Render**

Usar el Dockerfile original, pero Ollama local no se usar√° (va directo al externo).

---

## üìä COMPARACI√ìN DE COSTOS

| Opci√≥n | Costo Mensual | Precisi√≥n | Complejidad |
|--------|---------------|-----------|-------------|
| **Ollama 3B en Render** | $7 | 75-85% | Media |
| **Solo OpenAI** | $10-17 | 90-95% | Baja |
| **H√≠brido (Externo)** | $15-20 | 85-95% | Alta |

---

## üîß TROUBLESHOOTING

### Problema 1: Ollama no descarga el modelo

**S√≠ntoma:** Logs muestran "‚ö†Ô∏è No se pudo descargar modelo"

**Soluci√≥n:**
- Es normal en primera vez (timeout)
- El sistema usa fallback (OpenAI o Patterns)
- En siguiente deploy, el modelo ya estar√° descargado

### Problema 2: OOM (Out of Memory)

**S√≠ntoma:** Contenedor crashea con "Killed"

**Soluci√≥n:**
- Cambiar a modelo m√°s peque√±o: `llama3.1:1b`
- O cambiar a OPCI√ìN 2 (Solo OpenAI)

### Problema 3: Build timeout

**S√≠ntoma:** Build tarda m√°s de 15 minutos

**Soluci√≥n:**
- Descargar modelo despu√©s del deploy (no en Dockerfile)
- Modificar start.sh para descargar en background

### Problema 4: Cold starts lentos

**S√≠ntoma:** Primera request tarda 30-60 segundos

**Soluci√≥n:**
- Render Starter tiene cold starts
- Considerar plan Pro ($25/mes) con always-on
- O ping cada 10 minutos con cron job externo

---

## üìà MONITOREAR PRECISI√ìN

Una vez desplegado, monitorea las m√©tricas:

```bash
# Ver dashboard
https://aponntsuites.onrender.com/auditor-metrics.html

# API de m√©tricas
curl -H "Authorization: Bearer <token>" \
  https://aponntsuites.onrender.com/api/audit/metrics/dashboard-summary
```

**Criterio de decisi√≥n:**

- Si Ollama `success_rate < 70%` ‚Üí Cambiar a OpenAI
- Si Ollama `success_rate >= 75%` ‚Üí Mantener
- Revisar m√©tricas cada semana

---

## üéØ RECOMENDACI√ìN FINAL

Para tu caso (**Render Starter + $7/mes**):

1. **Empezar con OPCI√ìN 1** (Ollama 3B en Render)
2. **Monitorear m√©tricas** durante 1 semana
3. **Si precisi√≥n < 70%** ‚Üí Cambiar a OPCI√ìN 2 (OpenAI)
4. **Si precisi√≥n >= 75%** ‚Üí Mantener

**Costo total estimado:**
- OPCI√ìN 1: $7/mes (solo Render)
- Si cambias a OPCI√ìN 2: $10-17/mes (Render + OpenAI)

---

## üìù CHECKLIST DE DEPLOY

- [ ] Dockerfile creado
- [ ] .dockerignore configurado
- [ ] Variables de entorno configuradas en Render
- [ ] Migraci√≥n de BD ejecutada
- [ ] Repositorio pusheado a main
- [ ] Servicio creado en Render
- [ ] Build exitoso
- [ ] Health check funciona
- [ ] Dashboard de m√©tricas accesible
- [ ] Ejecutar primera auditor√≠a de prueba

---

**¬øListo para hacer deploy?**

1. Haz commit de todos los cambios
2. Push a GitHub/GitLab
3. Crea servicio en Render
4. Monitorea logs
5. ¬°Disfruta del sistema h√≠brido! üéâ
