# ðŸ¤– InstalaciÃ³n de Ollama + Llama 3.1

## Â¿QuÃ© es Ollama?
Ollama es un servidor de inferencia de IA que permite ejecutar modelos de lenguaje (LLMs) localmente en tu servidor/PC sin necesidad de APIs externas como OpenAI.

**Ventajas:**
- âœ… 100% privado (tus datos no salen del servidor)
- âœ… $0 de costo (sin suscripciones mensuales)
- âœ… Sin lÃ­mites de requests
- âœ… Latencia baja (servidor local)

**Requisitos de Hardware:**
- **MÃ­nimo:** 8 GB RAM, CPU moderna (4+ cores)
- **Recomendado:** 16 GB RAM, GPU NVIDIA (opcional pero acelera 10x)
- **Disco:** 5-10 GB por modelo

---

## OpciÃ³n 1: InstalaciÃ³n en Windows (Desarrollo Local)

### Paso 1: Descargar Ollama
```bash
# Visita https://ollama.com/download
# Descarga: OllamaSetup.exe para Windows
# Ejecuta el instalador
```

### Paso 2: Verificar instalaciÃ³n
```bash
# Abre PowerShell o CMD
ollama --version
# DeberÃ­a mostrar: ollama version 0.1.x
```

### Paso 3: Descargar modelo Llama 3.1
```bash
# Modelo pequeÃ±o (8B parÃ¡metros) - Recomendado para iniciar
ollama pull llama3.1:8b

# Modelo grande (70B parÃ¡metros) - Solo si tenÃ©s 32GB+ RAM
# ollama pull llama3.1:70b
```

Esto descargarÃ¡ ~4.7 GB (puede tardar 10-30 min segÃºn conexiÃ³n).

### Paso 4: Probar el modelo
```bash
# Test bÃ¡sico
ollama run llama3.1:8b "Â¿QuÃ© es un sistema de asistencia biomÃ©trico?"

# El modelo deberÃ­a responder en espaÃ±ol
```

### Paso 5: Verificar servidor
```bash
# Ollama inicia automÃ¡ticamente en http://localhost:11434
# Probar endpoint:
curl http://localhost:11434/api/tags

# DeberÃ­a retornar JSON con modelos instalados
```

---

## OpciÃ³n 2: InstalaciÃ³n en Linux (Render/VPS)

### Paso 1: Instalar Ollama vÃ­a script
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Paso 2: Iniciar servicio
```bash
# Ollama se instala como servicio systemd
sudo systemctl start ollama
sudo systemctl enable ollama  # Auto-iniciar en boot

# Verificar estado
sudo systemctl status ollama
```

### Paso 3: Descargar modelo
```bash
ollama pull llama3.1:8b
```

### Paso 4: Verificar
```bash
curl http://localhost:11434/api/tags
```

---

## OpciÃ³n 3: InstalaciÃ³n en Render (Cloud)

**âš ï¸ IMPORTANTE:** Render Free Tier NO soporta Ollama (falta de RAM y disco persistente).

**Requisitos para Render:**
- Plan: **Standard o superior** (~$25/mes mÃ­nimo)
- RAM: **16 GB mÃ­nimo**
- Disco: **25 GB mÃ­nimo**
- Persistent Disk habilitado

### ConfiguraciÃ³n en Render:

1. **Crear Web Service** con Docker:
```dockerfile
# Dockerfile.ollama
FROM ollama/ollama:latest

# Descargar modelo en build time
RUN ollama serve & sleep 5 && ollama pull llama3.1:8b

EXPOSE 11434
CMD ["ollama", "serve"]
```

2. **Environment Variables:**
```bash
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MODELS=/opt/ollama/models  # Debe ser persistent disk
```

3. **Health Check:**
```
Path: /api/tags
Port: 11434
```

**Costo estimado Render con Ollama:**
- Standard Plan: $25/mes (8GB RAM) - Justo para llama3.1:8b
- Pro Plan: $85/mes (16GB RAM) - CÃ³modo para llama3.1:8b
- Pro Plus: $250/mes (32GB RAM) - Para llama3.1:70b

---

## ConfiguraciÃ³n en el Sistema de Asistencia

### Variables de entorno (`.env`):
```bash
# URL del servidor Ollama
OLLAMA_BASE_URL=http://localhost:11434

# En producciÃ³n (Render):
# OLLAMA_BASE_URL=https://your-ollama-service.onrender.com

# Modelo a usar
OLLAMA_MODEL=llama3.1:8b

# Timeout para requests (en ms)
OLLAMA_TIMEOUT=30000

# Temperatura (0.0 = determinÃ­stico, 1.0 = creativo)
OLLAMA_TEMPERATURE=0.7

# Max tokens en respuesta
OLLAMA_MAX_TOKENS=500
```

---

## API de Ollama - Endpoints Principales

### 1. Generar respuesta (Chat)
```bash
POST http://localhost:11434/api/chat

{
  "model": "llama3.1:8b",
  "messages": [
    {
      "role": "system",
      "content": "Eres un asistente experto en sistemas de RRHH."
    },
    {
      "role": "user",
      "content": "Â¿CÃ³mo registro asistencias?"
    }
  ],
  "stream": false
}
```

### 2. Listar modelos instalados
```bash
GET http://localhost:11434/api/tags
```

### 3. Verificar salud del servidor
```bash
GET http://localhost:11434/
```

---

## Troubleshooting

### Problema: "connection refused"
**Causa:** Ollama no estÃ¡ corriendo
**SoluciÃ³n:**
```bash
# Windows: Abrir Ollama desde menÃº inicio
# Linux: sudo systemctl start ollama
```

### Problema: "model not found"
**Causa:** Modelo no descargado
**SoluciÃ³n:**
```bash
ollama pull llama3.1:8b
```

### Problema: Respuestas lentas (>10 seg)
**Causa:** CPU sin GPU, modelo muy grande
**SoluciÃ³n:**
- Usar modelo mÃ¡s pequeÃ±o: `llama3.1:8b` en vez de `70b`
- Agregar GPU NVIDIA compatible con CUDA
- Reducir `max_tokens` en configuraciÃ³n

### Problema: Out of memory
**Causa:** RAM insuficiente
**SoluciÃ³n:**
- Cerrar otros programas
- Usar modelo mÃ¡s pequeÃ±o
- Aumentar RAM del servidor

---

## RecomendaciÃ³n Final

**Para desarrollo local (Windows):**
âœ… Instalar Ollama en tu PC
âœ… Usar llama3.1:8b (4.7 GB)
âœ… Backend se conecta a http://localhost:11434

**Para producciÃ³n:**
ðŸ”„ **OpciÃ³n A:** VPS dedicado (DigitalOcean, Linode, etc.) con 16GB RAM (~$50/mes)
ðŸ”„ **OpciÃ³n B:** Render Standard + Persistent Disk (~$25-50/mes)
ðŸ”„ **OpciÃ³n C:** Migrar a OpenAI API cuando escale (solo pagar por uso)

---

## Next Steps

Una vez instalado Ollama:
1. âœ… Verificar que http://localhost:11434 responde
2. âœ… Confirmar modelo descargado: `ollama list`
3. âœ… Probar chat simple: `ollama run llama3.1:8b "Hola"`
4. âœ… Configurar `.env` con OLLAMA_BASE_URL
5. âœ… Continuar con implementaciÃ³n de AssistantService.js

**Status:** InstalaciÃ³n completada â†’ Avanzar a backend integration
