#
# ⚡ APACHE KAFKA CLUSTER PROFESSIONAL - FASE 5
# ==============================================
# Apache Kafka para streaming de datos biométricos en tiempo real
# Alta throughput, persistencia, particionado automático
# Fecha: 2025-09-26
# Versión: 2.0.0
#

apiVersion: v1
kind: Namespace
metadata:
  name: kafka
  labels:
    app: kafka-cluster
    version: v3.5.0

---
# ConfigMap para configuración de Kafka
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
  namespace: kafka
data:
  server.properties: |
    # Broker ID will be set dynamically
    broker.id=-1

    # Listeners
    listeners=PLAINTEXT://0.0.0.0:9092,INTERNAL://0.0.0.0:9093
    advertised.listeners=PLAINTEXT://${POD_NAME}.kafka-cluster.kafka.svc.cluster.local:9092,INTERNAL://${POD_NAME}.kafka-cluster.kafka.svc.cluster.local:9093
    listener.security.protocol.map=PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
    inter.broker.listener.name=INTERNAL

    # Zookeeper
    zookeeper.connect=zk-0.zookeeper.kafka.svc.cluster.local:2181,zk-1.zookeeper.kafka.svc.cluster.local:2181,zk-2.zookeeper.kafka.svc.cluster.local:2181
    zookeeper.connection.timeout.ms=18000

    # Log configuration
    log.dirs=/kafka-logs
    num.network.threads=8
    num.io.threads=16
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600

    # Log retention
    log.retention.hours=168
    log.retention.bytes=1073741824
    log.segment.bytes=1073741824
    log.retention.check.interval.ms=300000

    # Biometric-specific optimizations
    num.partitions=6
    default.replication.factor=3
    min.insync.replicas=2

    # Performance tuning
    replica.fetch.max.bytes=1048576
    message.max.bytes=1000012
    replica.socket.timeout.ms=30000
    replica.socket.receive.buffer.bytes=65536

    # Compression
    compression.type=snappy

    # JVM performance
    group.initial.rebalance.delay.ms=3000

---
# Headless Service para Kafka
apiVersion: v1
kind: Service
metadata:
  name: kafka-cluster
  namespace: kafka
  labels:
    app: kafka-cluster
spec:
  clusterIP: None
  selector:
    app: kafka-cluster
  ports:
  - port: 9092
    targetPort: 9092
    protocol: TCP
    name: kafka
  - port: 9093
    targetPort: 9093
    protocol: TCP
    name: kafka-internal

---
# Service para acceso externo
apiVersion: v1
kind: Service
metadata:
  name: kafka-service
  namespace: kafka
  labels:
    app: kafka-cluster
spec:
  selector:
    app: kafka-cluster
  ports:
  - port: 9092
    targetPort: 9092
    protocol: TCP
    name: kafka
  type: ClusterIP

---
# StatefulSet para Kafka
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-cluster
  namespace: kafka
  labels:
    app: kafka-cluster
spec:
  serviceName: kafka-cluster
  replicas: 3
  selector:
    matchLabels:
      app: kafka-cluster
  template:
    metadata:
      labels:
        app: kafka-cluster
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9308"
        prometheus.io/path: "/metrics"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - kafka-cluster
              topologyKey: kubernetes.io/hostname
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.5.0
        ports:
        - containerPort: 9092
          name: kafka
        - containerPort: 9093
          name: kafka-internal
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['kafka.broker.id']
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zk-0.zookeeper.kafka.svc.cluster.local:2181,zk-1.zookeeper.kafka.svc.cluster.local:2181,zk-2.zookeeper.kafka.svc.cluster.local:2181"
        - name: KAFKA_LISTENERS
          value: "PLAINTEXT://0.0.0.0:9092,INTERNAL://0.0.0.0:9093"
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://$(POD_NAME).kafka-cluster.kafka.svc.cluster.local:9092,INTERNAL://$(POD_NAME).kafka-cluster.kafka.svc.cluster.local:9093"
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: "PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT"
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: "INTERNAL"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_TRANSACTION_STATE_LOG_MIN_ISR
          value: "2"
        - name: KAFKA_DEFAULT_REPLICATION_FACTOR
          value: "3"
        - name: KAFKA_MIN_INSYNC_REPLICAS
          value: "2"
        - name: KAFKA_NUM_PARTITIONS
          value: "6"
        - name: KAFKA_COMPRESSION_TYPE
          value: "snappy"
        - name: KAFKA_LOG_RETENTION_HOURS
          value: "168"
        - name: KAFKA_LOG_RETENTION_BYTES
          value: "1073741824"
        - name: KAFKA_JVM_PERFORMANCE_OPTS
          value: "-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true"
        - name: KAFKA_HEAP_OPTS
          value: "-Xmx2G -Xms2G"
        resources:
          requests:
            memory: "3Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "1000m"
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        volumeMounts:
        - name: kafka-data
          mountPath: /kafka-logs
        - name: kafka-config
          mountPath: /etc/kafka

      # Kafka Exporter para métricas
      - name: kafka-exporter
        image: danielqsj/kafka-exporter:v1.6.0
        ports:
        - containerPort: 9308
          name: metrics
        args:
        - --kafka.server=localhost:9092
        - --web.listen-address=0.0.0.0:9308
        - --log.level=info
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

      volumes:
      - name: kafka-config
        configMap:
          name: kafka-config

  volumeClaimTemplates:
  - metadata:
      name: kafka-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ssd-retain
      resources:
        requests:
          storage: 100Gi

---
# Zookeeper StatefulSet (requerido por Kafka)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
  namespace: kafka
spec:
  serviceName: zookeeper
  replicas: 3
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - zookeeper
              topologyKey: kubernetes.io/hostname
      containers:
      - name: zookeeper
        image: confluentinc/cp-zookeeper:7.5.0
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        env:
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name: ZOOKEEPER_SERVER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['zookeeper.server.id']
        - name: ZOOKEEPER_SERVERS
          value: "zk-0.zookeeper.kafka.svc.cluster.local:2888:3888;zk-1.zookeeper.kafka.svc.cluster.local:2888:3888;zk-2.zookeeper.kafka.svc.cluster.local:2888:3888"
        - name: ZOOKEEPER_TICK_TIME
          value: "2000"
        - name: ZOOKEEPER_INIT_LIMIT
          value: "5"
        - name: ZOOKEEPER_SYNC_LIMIT
          value: "2"
        - name: ZOOKEEPER_HEAP_OPTS
          value: "-Xmx1G -Xms1G"
        resources:
          requests:
            memory: "1Gi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "500m"
        livenessProbe:
          tcpSocket:
            port: 2181
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 2181
          initialDelaySeconds: 10
          periodSeconds: 5
        volumeMounts:
        - name: zk-data
          mountPath: /var/lib/zookeeper/data
        - name: zk-logs
          mountPath: /var/lib/zookeeper/log
  volumeClaimTemplates:
  - metadata:
      name: zk-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ssd-retain
      resources:
        requests:
          storage: 10Gi
  - metadata:
      name: zk-logs
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ssd-retain
      resources:
        requests:
          storage: 10Gi

---
# Service para Zookeeper
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: kafka
spec:
  clusterIP: None
  selector:
    app: zookeeper
  ports:
  - port: 2181
    targetPort: 2181
    protocol: TCP
    name: client
  - port: 2888
    targetPort: 2888
    protocol: TCP
    name: server
  - port: 3888
    targetPort: 3888
    protocol: TCP
    name: leader-election

---
# Job para crear topics de biometría
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-topics-init
  namespace: kafka
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: kafka-topics-init
        image: confluentinc/cp-kafka:7.5.0
        command:
        - sh
        - -c
        - |
          echo "Waiting for Kafka to be ready..."
          sleep 120

          echo "Creating biometric topics..."

          # Templates topic
          kafka-topics --create --topic biometric.templates \
            --bootstrap-server kafka-cluster-0.kafka-cluster.kafka.svc.cluster.local:9092 \
            --partitions 6 --replication-factor 3 \
            --config compression.type=snappy \
            --config retention.ms=604800000

          # Analysis results topic
          kafka-topics --create --topic biometric.analysis \
            --bootstrap-server kafka-cluster-0.kafka-cluster.kafka.svc.cluster.local:9092 \
            --partitions 6 --replication-factor 3 \
            --config compression.type=snappy \
            --config retention.ms=2592000000

          # Real-time events topic
          kafka-topics --create --topic biometric.events \
            --bootstrap-server kafka-cluster-0.kafka-cluster.kafka.svc.cluster.local:9092 \
            --partitions 12 --replication-factor 3 \
            --config compression.type=snappy \
            --config retention.ms=86400000

          # Alerts topic
          kafka-topics --create --topic biometric.alerts \
            --bootstrap-server kafka-cluster-0.kafka-cluster.kafka.svc.cluster.local:9092 \
            --partitions 3 --replication-factor 3 \
            --config compression.type=snappy \
            --config retention.ms=2592000000

          # Metrics topic
          kafka-topics --create --topic biometric.metrics \
            --bootstrap-server kafka-cluster-0.kafka-cluster.kafka.svc.cluster.local:9092 \
            --partitions 6 --replication-factor 3 \
            --config compression.type=snappy \
            --config retention.ms=604800000

          echo "Biometric topics created successfully!"

---
# PodDisruptionBudget para Kafka
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: kafka-cluster-pdb
  namespace: kafka
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: kafka-cluster

---
# PodDisruptionBudget para Zookeeper
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zookeeper-pdb
  namespace: kafka
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: zookeeper

---
# ServiceMonitor para Kafka
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kafka-cluster-monitor
  namespace: kafka
  labels:
    app: kafka-cluster
spec:
  selector:
    matchLabels:
      app: kafka-cluster
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s

---
# NetworkPolicy para Kafka
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kafka-network-policy
  namespace: kafka
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: biometric-system
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
  - from:
    - podSelector: {}  # Permitir comunicación interna
  egress:
  - to:
    - podSelector: {}  # Permitir comunicación interna
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53